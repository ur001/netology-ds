{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание по обработке текстов\n",
    "\n",
    "## Предсказание цены акции по экономическим новостям\n",
    "\n",
    "Входные данные:\n",
    "* Новости о компании \"Газпром\", начиная с 2010 года\n",
    "* Стоимость акций компании \"Газпром\" на ММВБ, начиная с 2010 года\n",
    "    * цена открытия (Open)\n",
    "    * цена закрытия (ClosingPrice)\n",
    "    * максимальная цена за день (DailyHigh)\n",
    "    * минимальная цена за день (DailyLow) \n",
    "    * объем бумаг (VolumePcs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook as progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"./data\"\n",
    "random_state = 777\n",
    "\n",
    "%pylab inline\n",
    "%config InlineBackend.figure_format = 'svg' \n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# отключаем ворнинги\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# дефолтный размер графиков\n",
    "from pylab import rcParams\n",
    "# rcParams['figure.figsize'] = 14,9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "На одну дату только одна новость\n"
     ]
    }
   ],
   "source": [
    "# Проверяем, есть ли дубликаты новостей на один и тот же день\n",
    "news_dates = pd.read_csv(os.path.join(data_dir, 'texts.csv')).date.value_counts()\n",
    "print({\n",
    "    True: \"На одну дату может быть несколько новостей\",\n",
    "    False: \"На одну дату только одна новость\"\n",
    "}[len(news_dates[news_dates > 1]) > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    news = pd.read_csv(os.path.join(data_dir, 'texts.csv'))\n",
    "    prices = pd.read_csv(os.path.join(data_dir, 'gazprom_prices.csv'), sep=';', )\n",
    "    news.date = pd.to_datetime(news.date, format='%d.%m.%Y')\n",
    "    prices['date'] = pd.to_datetime(prices.Date, format='%d.%m.%Y')\n",
    "    prices.drop(['Date'], axis=1, inplace=True)\n",
    "    for column in ('Open','ClosingPrice', 'DailyHigh', 'DailyLow'):\n",
    "        prices[column] = prices[column].map(\n",
    "            lambda price: np.float32(price.replace(',', '.')),\n",
    "            na_action='ignore'\n",
    "        )\n",
    "    # Назначаем date индексом, проверено: дубликатов новостей на одну дату нет\n",
    "    return news.set_index('date'), prices.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-09</th>\n",
       "      <td>Компания рассчитывает на решение по газовому с...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-08</th>\n",
       "      <td>Как и предполагал “Ъ”, «Газпром», воспользова...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-01</th>\n",
       "      <td>Новая редакция американских санкций ставит по...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-30</th>\n",
       "      <td>Как стало известно “Ъ”, известный на рынке ри...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-23</th>\n",
       "      <td>НОВАТЭК, который через пять лет собирается за...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         text\n",
       "date                                                         \n",
       "2017-11-09  Компания рассчитывает на решение по газовому с...\n",
       "2017-11-08   Как и предполагал “Ъ”, «Газпром», воспользова...\n",
       "2017-11-01   Новая редакция американских санкций ставит по...\n",
       "2017-10-30   Как стало известно “Ъ”, известный на рынке ри...\n",
       "2017-10-23   НОВАТЭК, который через пять лет собирается за..."
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news, prices = read_data()\n",
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>ClosingPrice</th>\n",
       "      <th>DailyHigh</th>\n",
       "      <th>DailyLow</th>\n",
       "      <th>VolumePcs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-12-08</th>\n",
       "      <td>133.429993</td>\n",
       "      <td>132.600006</td>\n",
       "      <td>133.899994</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>16037970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-07</th>\n",
       "      <td>133.699997</td>\n",
       "      <td>133.020004</td>\n",
       "      <td>133.869995</td>\n",
       "      <td>132.809998</td>\n",
       "      <td>18198430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-06</th>\n",
       "      <td>133.330002</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>134.289993</td>\n",
       "      <td>132.910004</td>\n",
       "      <td>14641730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-05</th>\n",
       "      <td>133.479996</td>\n",
       "      <td>133.649994</td>\n",
       "      <td>133.990005</td>\n",
       "      <td>132.779999</td>\n",
       "      <td>12684800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-04</th>\n",
       "      <td>133.009995</td>\n",
       "      <td>133.770004</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>131.929993</td>\n",
       "      <td>17818980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open  ClosingPrice   DailyHigh    DailyLow  VolumePcs\n",
       "date                                                                   \n",
       "2017-12-08  133.429993    132.600006  133.899994  132.000000   16037970\n",
       "2017-12-07  133.699997    133.020004  133.869995  132.809998   18198430\n",
       "2017-12-06  133.330002    134.000000  134.289993  132.910004   14641730\n",
       "2017-12-05  133.479996    133.649994  133.990005  132.779999   12684800\n",
       "2017-12-04  133.009995    133.770004  134.000000  131.929993   17818980"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1. Вводная\n",
    "\n",
    "Проведите предобработку текстов: если считаете нужным, выполните токенизацию, приведение к нижнему регистру, лемматизацию и/или стемминг. Ответьте на следующие вопросы:\n",
    "* Есть ли корреляция между средней длинной текста за день и ценой закрытия?\n",
    "* Есть ли корреляция между количеством упоминаний Алексея Миллера  и ценой закрытия? Учтите разные варианты написания имени.\n",
    "* Упоминаний какого газопровода в статьях больше: \n",
    "    * \"северный поток\"\n",
    "    * \"турецкий поток\"?\n",
    "* О каких санкциях пишут в статьях?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Корелляция между средней длинной текста за день и ценой закрытия\n",
    "#### Вычисляем длины новостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvIAAAH0CAYAAABfKsnMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAIAOCfIAANAhQR4AADokyAMAQIf+P5fn/3IkTTjKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10fbb4ef0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 377
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "news['len'] = news.text.apply(len)\n",
    "news.len.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Проверяем, совпадают ли диапозоны дат новостей и котировок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Даты новостей: 2010.01.04 - 2017.11.09\n",
      "Даты цен: 2010.01.11 - 2017.12.08\n",
      "Даты с новостями и ценами: 2010.01.11 - 2017.11.09\n"
     ]
    }
   ],
   "source": [
    "news_date_from, news_date_to = news.index.min(), news.index.max()\n",
    "prices_date_from, prices_date_to = prices.index.min(), prices.index.max()\n",
    "date_from = max(news_date_from, prices_date_from)\n",
    "date_to = min(news_date_to, prices_date_to)\n",
    "print(\"Даты новостей: {:%Y.%m.%d} - {:%Y.%m.%d}\".format(news_date_from, news_date_to))\n",
    "print(\"Даты цен: {:%Y.%m.%d} - {:%Y.%m.%d}\".format(prices_date_from, prices_date_to))\n",
    "print(\"Даты с новостями и ценами: {:%Y.%m.%d} - {:%Y.%m.%d}\".format(date_from, date_to))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Формируем выборку по датам на которые есть цена и новость"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ClosingPrice</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-09</th>\n",
       "      <td>131.500000</td>\n",
       "      <td>419.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-08</th>\n",
       "      <td>132.300003</td>\n",
       "      <td>624.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-07</th>\n",
       "      <td>132.279999</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-03</th>\n",
       "      <td>125.900002</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-02</th>\n",
       "      <td>125.669998</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ClosingPrice    len\n",
       "date                           \n",
       "2017-11-09    131.500000  419.0\n",
       "2017-11-08    132.300003  624.0\n",
       "2017-11-07    132.279999    0.0\n",
       "2017-11-03    125.900002    0.0\n",
       "2017-11-02    125.669998    0.0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_common_dates(prices, news, date_from, date_to):\n",
    "    return (\n",
    "        prices.loc[(prices.index >= date_from) & (prices.index <= date_to)],\n",
    "        news.loc[(news.index >= date_from) & (news.index <= date_to)]\n",
    "    )\n",
    "\n",
    "common_prices, common_news = filter_common_dates(prices, news, date_from, date_to)\n",
    "closing_prices = common_prices[['ClosingPrice']]\n",
    "prices_vs_news_len = closing_prices.join(common_news.len).fillna(0)\n",
    "prices_vs_news_len.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Корреляция цены закрытия с длиной текста и против случайного массива"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.05536274198156127, 0.014060710868536922)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Корреляция сдлиной текста новости\n",
    "pearsonr(prices_vs_news_len.ClosingPrice, prices_vs_news_len.len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.046566073507463883, 0.03891794326819277)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Корреляция со случайным вектором такой же длины\n",
    "random_vals = np.random.rand(len(prices_vs_news_len.ClosingPrice))\n",
    "pearsonr(prices_vs_news_len.ClosingPrice, random_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** корреляция отсутствует"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Очистка текста и лематизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 475 ms, sys: 50.8 ms, total: 526 ms\n",
      "Wall time: 6.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from pymystem3 import Mystem\n",
    "\n",
    "def lemmatize(text, lemmatizer):\n",
    "    text = text.replace('\\xa0', ' ')\n",
    "    try:\n",
    "        return \"\".join(lemmatizer.lemmatize(text)).strip()  \n",
    "    except:\n",
    "        return \" \"\n",
    "\n",
    "news['text_norm'] = news.text.apply(lemmatize, lemmatizer=Mystem())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Корреляция между числом упоминанием Миллера и ценой закрытия\n",
    "#### Считаем число упоминаний"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news['miller_count'] = news.text_norm.map(\n",
    "    lambda text: text.count('миллер') if text else 0,\n",
    "    na_action='ignore'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \"Газпром\" принял решение о целесообразности строительства еще двух ниток \"Северного потока\"; одна из новых веток может дойти до Великобритании, состав участников проекта ее строительства может отличаться от нынешнего состава акционеров Nord Stream AG, сообщил председатель правления \"Газпрома\" Алексей ◀Миллер▶ на пресс-конференции в пятницу.\n",
      " \"Газпром\" принял решение о целесообразности строительства еще двух ниток \"Северного потока\"; одна из новых веток может дойти до Великобритании, состав участников проекта ее строительства может отличаться от нынешнего состава акционеров Nord Stream AG, сообщил председатель правления \"Газпрома\" Алексей ◀Миллер▶ на пресс-конференции в пятницу.\n",
      " \"Газпром\" в 2013 г. может рассмотреть возможность выплаты дивидендов в 25% чистой прибыли по МСФО, сообщил председатель правления российского газового холдинга Алексей ◀Миллер▶ на пресс-конференции в пятницу. Минфин России полагает, что госкомпании должны платить в качестве дивидендов не менее 25% чистой прибыли по МСФО.\n",
      " \"Газпром\" планирует ввести в строй Бованенковское месторождение природного газа 22 октября 2012 г., сообщил журналистам председатель правления холдинга Алексей ◀Миллер▶. Ранее \"Газпром\" планировал начать там добычу в начале июня. \"Поедем [на месторождение] 22 октября\", - сказал ◀Миллер▶.\n",
      " \"Газпром\" не планирует выкупа собственных акций с рынка, сообщил председатель правления газового холдинга Алексей ◀Миллер▶ на пресс-конференции в пятницу по итогам общего годового собрания акционеров. \"В настоящее время \"Газпром\" не рассматривает для себя такой возможности\", - сказал он, отвечая на вопрос о возможности осуществления холдингом buy back.\n",
      " \"Газпром\" оказался заложником территориального спора Китая и Вьетнама в Южно-Китайском море. Китайская нефтекомпания CNOOC предлагает иностранным инвесторам начать работы на шельфе Вьетнама на участках, лицензии на которые уже выданы \"Газпрому\". Результатом этого могут стать конфликт \"Газпрома\" с китайской стороной и еще большее затягивание проекта поставок российского газа в Китай.\n"
     ]
    }
   ],
   "source": [
    "print(news.sort_values(by=['miller_count'], ascending=False).head(1).text[0].replace(\n",
    "    'Миллер', '◀Миллер▶'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Считаем корреляцию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ClosingPrice</th>\n",
       "      <th>miller_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-06-30</th>\n",
       "      <td>139.509995</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-06-26</th>\n",
       "      <td>145.179993</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-10-07</th>\n",
       "      <td>137.100006</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-06-16</th>\n",
       "      <td>145.199997</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-09-06</th>\n",
       "      <td>141.970001</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ClosingPrice  miller_count\n",
       "date                                  \n",
       "2016-06-30    139.509995           3.0\n",
       "2015-06-26    145.179993           4.0\n",
       "2014-10-07    137.100006           3.0\n",
       "2014-06-16    145.199997           3.0\n",
       "2013-09-06    141.970001           3.0"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_prices, common_news = filter_common_dates(prices, news, date_from, date_to)\n",
    "prices_vs_miller_counts = closing_prices.join(common_news.miller_count).fillna(0)\n",
    "prices_vs_miller_counts[prices_vs_miller_counts.miller_count > 2].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    1868\n",
       "1.0      69\n",
       "2.0      19\n",
       "3.0       7\n",
       "4.0       2\n",
       "5.0       1\n",
       "6.0       1\n",
       "Name: miller_count, dtype: int64"
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices_vs_miller_counts.miller_count.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.007440481290018384, 0.74156117961881796)"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Корреляция цены акций на момент закрытия с числом упоминания Миллера в новостях\n",
    "pearsonr(prices_vs_miller_counts.ClosingPrice, prices_vs_miller_counts.miller_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.00744048],\n",
       "       [-0.00744048,  1.        ]])"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(\n",
    "    prices_vs_miller_counts.ClosingPrice.tolist(), \n",
    "    prices_vs_miller_counts.miller_count.tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** корреляция отсутствует"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Упоминания в новостях северного и турецкого потока (газопроводы)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Северный поток: 15\n",
      "Турецкий поток: 39\n"
     ]
    }
   ],
   "source": [
    "print(\"Северный поток: {}\\nТурецкий поток: {}\".format(\n",
    "    sum(news.text_norm.map(lambda text: text.count('северный поток'))),\n",
    "    sum(news.text_norm.map(lambda text: text.count('турецкий поток')))\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** упоминаний «турецкого потока» в 2.5 раза больше, чем «северного»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### О каких санкциях пишут в статьях?\n",
    "#### Парсим новости с помощью SyntaxNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_texts(texts, file_name):\n",
    "    with open(file_name, 'w') as f:\n",
    "        for text in texts:\n",
    "            f.write(\"{}\\n\".format(\n",
    "                text.replace('\\n', ' ').replace('\\r', '').replace('\\t', ' ')\n",
    "            ))\n",
    "\n",
    "save_texts(news.text, 'news_texts.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-12-17 15:08:29.835639: W external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n",
      "2017-12-17 15:08:29.835639: W external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n",
      "2017-12-17 15:08:29.835639: W external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n",
      "2017-12-17 15:08:29.835772: W external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n",
      "2017-12-17 15:08:29.835791: W external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n",
      "2017-12-17 15:08:29.835811: W external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n",
      "2017-12-17 15:08:29.835819: W external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n",
      "2017-12-17 15:08:29.835772: W external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n",
      "2017-12-17 15:08:29.835870: W external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n",
      "2017-12-17 15:08:29.835928: W external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n",
      "2017-12-17 15:08:29.835880: W external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n",
      "2017-12-17 15:08:29.836093: W external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n",
      "2017-12-17 15:08:29.836302: W external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n",
      "2017-12-17 15:08:29.836336: W external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n",
      "2017-12-17 15:08:29.836366: W external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n",
      "2017-12-17 15:08:29.852226: I syntaxnet/term_frequency_map.cc:101] Loaded 34 terms from ./syntaxnet/models/Russian-SynTagRus/label-map.\r\n",
      "2017-12-17 15:08:29.852226: I syntaxnet/term_frequency_map.cc:101] Loaded 34 terms from ./syntaxnet/models/Russian-SynTagRus/label-map.\r\n",
      "2017-12-17 15:08:29.852634: I syntaxnet/embedding_feature_extractor.cc:35] Features: input.capitalization input(1).capitalization input(2).capitalization input(3).capitalization input(-1).capitalization input(-2).capitalization input(-3).capitalization input(-4).capitalization; input.token.char-ngram input(1).token.char-ngram input(2).token.char-ngram input(3).token.char-ngram input(-1).token.char-ngram input(-2).token.char-ngram input(-3).token.char-ngram input(-4).token.char-ngram; input.digit input.hyphen input.token.punctuation-amount input.token.quote; input.token.prefix(length=2) input(1).token.prefix(length=2) input(2).token.prefix(length=2) input(3).token.prefix(length=2) input(-1).token.prefix(length=2) input(-2).token.prefix(length=2) input(-3).token.prefix(length=2) input(-4).token.prefix(length=2); input.token.prefix(length=3) input(1).token.prefix(length=3) input(2).token.prefix(length=3) input(3).token.prefix(length=3) input(-1).token.prefix(length=3) input(-2).token.prefix(length=3) input(-3).token.prefix(length=3) input(-4).token.prefix(length=3); input.token.suffix(length=2) input(1).token.suffix(length=2) input(2).token.suffix(length=2) input(3).token.suffix(length=2) input(-1).token.suffix(length=2) input(-2).token.suffix(length=2) input(-3).token.suffix(length=2) input(-4).token.suffix(length=2); input.token.suffix(length=3) input(1).token.suffix(length=3) input(2).token.suffix(length=3) input(3).token.suffix(length=3) input(-1).token.suffix(length=3) input(-2).token.suffix(length=3) input(-3).token.suffix(length=3) input(-4).token.suffix(length=3); input(-1).pred-morph-tag input(-2).pred-morph-tag input(-3).pred-morph-tag input(-4).pred-morph-tag; input.token.word input(1).token.word input(2).token.word input(3).token.word input(-1).token.word input(-2).token.word input(-3).token.word input(-4).token.word\r\n",
      "2017-12-17 15:08:29.852682: I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: capitalization;char_ngram;other;prefix2;prefix3;suffix2;suffix3;tags;words\r\n",
      "2017-12-17 15:08:29.852650: I syntaxnet/embedding_feature_extractor.cc:35] Features: stack.child(1).label stack.child(1).sibling(-1).label stack.child(-1).label stack.child(-1).sibling(1).label stack.child(2).label stack.child(-2).label stack(1).child(1).label stack(1).child(1).sibling(-1).label stack(1).child(-1).label stack(1).child(-1).sibling(1).label stack(1).child(2).label stack(1).child(-2).label; input.token.morphology-set input(1).token.morphology-set input(2).token.morphology-set input(3).token.morphology-set stack.token.morphology-set stack.child(1).token.morphology-set stack.child(1).sibling(-1).token.morphology-set stack.child(-1).token.morphology-set stack.child(-1).sibling(1).token.morphology-set stack.child(2).token.morphology-set stack.child(-2).token.morphology-set stack(1).token.morphology-set stack(1).child(1).token.morphology-set stack(1).child(1).sibling(-1).token.morphology-set stack(1).child(-1).token.morphology-set stack(1).child(-1).sibling(1).token.morphology-set stack(1).child(2).token.morphology-set stack(1).child(-2).token.morphology-set stack(2).token.morphology-set stack(3).token.morphology-set; input.token.tag input(1).token.tag input(2).token.tag input(3).token.tag stack.token.tag stack.child(1).token.tag stack.child(1).sibling(-1).token.tag stack.child(-1).token.tag stack.child(-1).sibling(1).token.tag stack.child(2).token.tag stack.child(-2).token.tag stack(1).token.tag stack(1).child(1).token.tag stack(1).child(1).sibling(-1).token.tag stack(1).child(-1).token.tag stack(1).child(-1).sibling(1).token.tag stack(1).child(2).token.tag stack(1).child(-2).token.tag stack(2).token.tag stack(3).token.tag; input.token.word input(1).token.word input(2).token.word input(3).token.word stack.token.word stack.child(1).token.word stack.child(1).sibling(-1).token.word stack.child(-1).token.word stack.child(-1).sibling(1).token.word stack.child(2).token.word stack.child(-2).token.word stack(1).token.word stack(1).child(1).token.word stack(1).child(1).sibling(-1).token.word stack(1).child(-1).token.word stack(1).child(-1).sibling(1).token.word stack(1).child(2).token.word stack(1).child(-2).token.word stack(2).token.word stack(3).token.word \r\n",
      "2017-12-17 15:08:29.852701: I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: 2;16;8;16;16;16;16;16;64\r\n",
      "2017-12-17 15:08:29.852712: I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: labels;morphology;tags;words\r\n",
      "2017-12-17 15:08:29.852762: I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: 32;32;32;64\r\n",
      "2017-12-17 15:08:29.854001: I syntaxnet/term_frequency_map.cc:101] Loaded 34 terms from ./syntaxnet/models/Russian-SynTagRus/label-map.\r\n",
      "2017-12-17 15:08:29.854556: I syntaxnet/term_frequency_map.cc:101] Loaded 66 terms from ./syntaxnet/models/Russian-SynTagRus/morphology-map.\r\n",
      "2017-12-17 15:08:29.854926: I syntaxnet/embedding_feature_extractor.cc:35] Features: input.capitalization input(1).capitalization input(2).capitalization input(3).capitalization input(-1).capitalization input(-2).capitalization input(-3).capitalization input(-4).capitalization; input.token.char-ngram input(1).token.char-ngram input(2).token.char-ngram input(3).token.char-ngram input(-1).token.char-ngram input(-2).token.char-ngram input(-3).token.char-ngram input(-4).token.char-ngram; input.digit input.hyphen input.token.punctuation-amount input.token.quote; input.token.prefix(length=2) input(1).token.prefix(length=2) input(2).token.prefix(length=2) input(3).token.prefix(length=2) input(-1).token.prefix(length=2) input(-2).token.prefix(length=2) input(-3).token.prefix(length=2) input(-4).token.prefix(length=2); input.token.prefix(length=3) input(1).token.prefix(length=3) input(2).token.prefix(length=3) input(3).token.prefix(length=3) input(-1).token.prefix(length=3) input(-2).token.prefix(length=3) input(-3).token.prefix(length=3) input(-4).token.prefix(length=3); input.token.suffix(length=2) input(1).token.suffix(length=2) input(2).token.suffix(length=2) input(3).token.suffix(length=2) input(-1).token.suffix(length=2) input(-2).token.suffix(length=2) input(-3).token.suffix(length=2) input(-4).token.suffix(length=2); input.token.suffix(length=3) input(1).token.suffix(length=3) input(2).token.suffix(length=3) input(3).token.suffix(length=3) input(-1).token.suffix(length=3) input(-2).token.suffix(length=3) input(-3).token.suffix(length=3) input(-4).token.suffix(length=3); input(-1).pred-tag input(-2).pred-tag input(-3).pred-tag input(-4).pred-tag; input.token.word input(1).token.word input(2).token.word input(3).token.word input(-1).token.word input(-2).token.word input(-3).token.word input(-4).token.word\r\n",
      "2017-12-17 15:08:29.855069: I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: capitalization;char_ngram;other;prefix2;prefix3;suffix2;suffix3;tags;words\r\n",
      "2017-12-17 15:08:29.855304: I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: 2;16;8;16;16;16;16;16;64\r\n",
      "2017-12-17 15:08:29.855508: I syntaxnet/term_frequency_map.cc:101] Loaded 31 terms from ./syntaxnet/models/Russian-SynTagRus/tag-map.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-12-17 15:08:30.120884: I syntaxnet/term_frequency_map.cc:101] Loaded 18749 terms from ./syntaxnet/models/Russian-SynTagRus/char-ngram-map.\n",
      "2017-12-17 15:08:30.138862: I syntaxnet/term_frequency_map.cc:101] Loaded 18749 terms from ./syntaxnet/models/Russian-SynTagRus/char-ngram-map.\n",
      "2017-12-17 15:08:30.149877: I syntaxnet/term_frequency_map.cc:101] Loaded 31 terms from ./syntaxnet/models/Russian-SynTagRus/tag-map.\n",
      "2017-12-17 15:08:31.290279: I syntaxnet/term_frequency_map.cc:101] Loaded 103473 terms from ./syntaxnet/models/Russian-SynTagRus/word-map.\n",
      "INFO:tensorflow:Building training network with parameters: feature_sizes: [12 20 20 20] domain_sizes: [    37     66     33 103475]\n",
      "2017-12-17 15:08:31.529283: I syntaxnet/term_frequency_map.cc:101] Loaded 103473 terms from ./syntaxnet/models/Russian-SynTagRus/word-map.\n",
      "2017-12-17 15:08:31.536480: I syntaxnet/term_frequency_map.cc:101] Loaded 103473 terms from ./syntaxnet/models/Russian-SynTagRus/word-map.\n",
      "INFO:tensorflow:Building training network with parameters: feature_sizes: [8 8 4 8 8 8 8 4 8] domain_sizes: [     7  18750      5   8502   8502   7249   7249    449 103475]\n",
      "INFO:tensorflow:Building training network with parameters: feature_sizes: [8 8 4 8 8 8 8 4 8] domain_sizes: [     7  18750      5   8502   8502   7249   7249     34 103475]\n",
      "INFO:tensorflow:Restoring parameters from ./syntaxnet/models/Russian-SynTagRus/parser-params\n",
      "2017-12-17 15:08:32.499225: I syntaxnet/term_frequency_map.cc:101] Loaded 34 terms from ./syntaxnet/models/Russian-SynTagRus/label-map.\n",
      "2017-12-17 15:08:32.499323: I syntaxnet/embedding_feature_extractor.cc:35] Features: stack.child(1).label stack.child(1).sibling(-1).label stack.child(-1).label stack.child(-1).sibling(1).label stack.child(2).label stack.child(-2).label stack(1).child(1).label stack(1).child(1).sibling(-1).label stack(1).child(-1).label stack(1).child(-1).sibling(1).label stack(1).child(2).label stack(1).child(-2).label; input.token.morphology-set input(1).token.morphology-set input(2).token.morphology-set input(3).token.morphology-set stack.token.morphology-set stack.child(1).token.morphology-set stack.child(1).sibling(-1).token.morphology-set stack.child(-1).token.morphology-set stack.child(-1).sibling(1).token.morphology-set stack.child(2).token.morphology-set stack.child(-2).token.morphology-set stack(1).token.morphology-set stack(1).child(1).token.morphology-set stack(1).child(1).sibling(-1).token.morphology-set stack(1).child(-1).token.morphology-set stack(1).child(-1).sibling(1).token.morphology-set stack(1).child(2).token.morphology-set stack(1).child(-2).token.morphology-set stack(2).token.morphology-set stack(3).token.morphology-set; input.token.tag input(1).token.tag input(2).token.tag input(3).token.tag stack.token.tag stack.child(1).token.tag stack.child(1).sibling(-1).token.tag stack.child(-1).token.tag stack.child(-1).sibling(1).token.tag stack.child(2).token.tag stack.child(-2).token.tag stack(1).token.tag stack(1).child(1).token.tag stack(1).child(1).sibling(-1).token.tag stack(1).child(-1).token.tag stack(1).child(-1).sibling(1).token.tag stack(1).child(2).token.tag stack(1).child(-2).token.tag stack(2).token.tag stack(3).token.tag; input.token.word input(1).token.word input(2).token.word input(3).token.word stack.token.word stack.child(1).token.word stack.child(1).sibling(-1).token.word stack.child(-1).token.word stack.child(-1).sibling(1).token.word stack.child(2).token.word stack.child(-2).token.word stack(1).token.word stack(1).child(1).token.word stack(1).child(1).sibling(-1).token.word stack(1).child(-1).token.word stack(1).child(-1).sibling(1).token.word stack(1).child(2).token.word stack(1).child(-2).token.word stack(2).token.word stack(3).token.word \n",
      "2017-12-17 15:08:32.499356: I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: labels;morphology;tags;words\n",
      "2017-12-17 15:08:32.499376: I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: 32;32;32;64\n",
      "2017-12-17 15:08:32.500663: I syntaxnet/term_frequency_map.cc:101] Loaded 66 terms from ./syntaxnet/models/Russian-SynTagRus/morphology-map.\n",
      "2017-12-17 15:08:32.501309: I syntaxnet/term_frequency_map.cc:101] Loaded 31 terms from ./syntaxnet/models/Russian-SynTagRus/tag-map.\n",
      "INFO:tensorflow:Restoring parameters from ./syntaxnet/models/Russian-SynTagRus/morpher-params\n",
      "INFO:tensorflow:Restoring parameters from ./syntaxnet/models/Russian-SynTagRus/tagger-params\n",
      "2017-12-17 15:08:33.131183: I syntaxnet/term_frequency_map.cc:101] Loaded 31 terms from ./syntaxnet/models/Russian-SynTagRus/tag-map.\n",
      "2017-12-17 15:08:33.132345: I syntaxnet/term_frequency_map.cc:101] Loaded 34 terms from ./syntaxnet/models/Russian-SynTagRus/label-map.\n",
      "2017-12-17 15:08:33.132426: I syntaxnet/embedding_feature_extractor.cc:35] Features: input.capitalization input(1).capitalization input(2).capitalization input(3).capitalization input(-1).capitalization input(-2).capitalization input(-3).capitalization input(-4).capitalization; input.token.char-ngram input(1).token.char-ngram input(2).token.char-ngram input(3).token.char-ngram input(-1).token.char-ngram input(-2).token.char-ngram input(-3).token.char-ngram input(-4).token.char-ngram; input.digit input.hyphen input.token.punctuation-amount input.token.quote; input.token.prefix(length=2) input(1).token.prefix(length=2) input(2).token.prefix(length=2) input(3).token.prefix(length=2) input(-1).token.prefix(length=2) input(-2).token.prefix(length=2) input(-3).token.prefix(length=2) input(-4).token.prefix(length=2); input.token.prefix(length=3) input(1).token.prefix(length=3) input(2).token.prefix(length=3) input(3).token.prefix(length=3) input(-1).token.prefix(length=3) input(-2).token.prefix(length=3) input(-3).token.prefix(length=3) input(-4).token.prefix(length=3); input.token.suffix(length=2) input(1).token.suffix(length=2) input(2).token.suffix(length=2) input(3).token.suffix(length=2) input(-1).token.suffix(length=2) input(-2).token.suffix(length=2) input(-3).token.suffix(length=2) input(-4).token.suffix(length=2); input.token.suffix(length=3) input(1).token.suffix(length=3) input(2).token.suffix(length=3) input(3).token.suffix(length=3) input(-1).token.suffix(length=3) input(-2).token.suffix(length=3) input(-3).token.suffix(length=3) input(-4).token.suffix(length=3); input(-1).pred-tag input(-2).pred-tag input(-3).pred-tag input(-4).pred-tag; input.token.word input(1).token.word input(2).token.word input(3).token.word input(-1).token.word input(-2).token.word input(-3).token.word input(-4).token.word\n",
      "2017-12-17 15:08:33.132454: I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: capitalization;char_ngram;other;prefix2;prefix3;suffix2;suffix3;tags;words\n",
      "2017-12-17 15:08:33.132798: I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: 2;16;8;16;16;16;16;16;64\n",
      "2017-12-17 15:08:33.133128: I syntaxnet/term_frequency_map.cc:101] Loaded 34 terms from ./syntaxnet/models/Russian-SynTagRus/label-map.\n",
      "2017-12-17 15:08:33.133186: I syntaxnet/embedding_feature_extractor.cc:35] Features: input.capitalization input(1).capitalization input(2).capitalization input(3).capitalization input(-1).capitalization input(-2).capitalization input(-3).capitalization input(-4).capitalization; input.token.char-ngram input(1).token.char-ngram input(2).token.char-ngram input(3).token.char-ngram input(-1).token.char-ngram input(-2).token.char-ngram input(-3).token.char-ngram input(-4).token.char-ngram; input.digit input.hyphen input.token.punctuation-amount input.token.quote; input.token.prefix(length=2) input(1).token.prefix(length=2) input(2).token.prefix(length=2) input(3).token.prefix(length=2) input(-1).token.prefix(length=2) input(-2).token.prefix(length=2) input(-3).token.prefix(length=2) input(-4).token.prefix(length=2); input.token.prefix(length=3) input(1).token.prefix(length=3) input(2).token.prefix(length=3) input(3).token.prefix(length=3) input(-1).token.prefix(length=3) input(-2).token.prefix(length=3) input(-3).token.prefix(length=3) input(-4).token.prefix(length=3); input.token.suffix(length=2) input(1).token.suffix(length=2) input(2).token.suffix(length=2) input(3).token.suffix(length=2) input(-1).token.suffix(length=2) input(-2).token.suffix(length=2) input(-3).token.suffix(length=2) input(-4).token.suffix(length=2); input.token.suffix(length=3) input(1).token.suffix(length=3) input(2).token.suffix(length=3) input(3).token.suffix(length=3) input(-1).token.suffix(length=3) input(-2).token.suffix(length=3) input(-3).token.suffix(length=3) input(-4).token.suffix(length=3); input(-1).pred-morph-tag input(-2).pred-morph-tag input(-3).pred-morph-tag input(-4).pred-morph-tag; input.token.word input(1).token.word input(2).token.word input(3).token.word input(-1).token.word input(-2).token.word input(-3).token.word input(-4).token.word\n",
      "2017-12-17 15:08:33.133210: I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: capitalization;char_ngram;other;prefix2;prefix3;suffix2;suffix3;tags;words\n",
      "2017-12-17 15:08:33.133243: I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: 2;16;8;16;16;16;16;16;64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-12-17 15:08:33.374214: I syntaxnet/term_frequency_map.cc:101] Loaded 18749 terms from ./syntaxnet/models/Russian-SynTagRus/char-ngram-map.\n",
      "2017-12-17 15:08:33.380874: I syntaxnet/term_frequency_map.cc:101] Loaded 18749 terms from ./syntaxnet/models/Russian-SynTagRus/char-ngram-map.\n",
      "2017-12-17 15:08:33.961484: I syntaxnet/term_frequency_map.cc:101] Loaded 103473 terms from ./syntaxnet/models/Russian-SynTagRus/word-map.\n",
      "2017-12-17 15:08:34.744235: I syntaxnet/term_frequency_map.cc:101] Loaded 103473 terms from ./syntaxnet/models/Russian-SynTagRus/word-map.\n",
      "2017-12-17 15:08:34.756067: I syntaxnet/term_frequency_map.cc:101] Loaded 103473 terms from ./syntaxnet/models/Russian-SynTagRus/word-map.\n",
      "INFO:tensorflow:Processed 1024 documents\n",
      "INFO:tensorflow:Processed 178 documents\n",
      "INFO:tensorflow:Processed 1024 documents\n",
      "INFO:tensorflow:Total processed documents: 1202\n",
      "INFO:tensorflow:num correct tokens: 0\n",
      "INFO:tensorflow:total tokens: 122137\n",
      "INFO:tensorflow:Seconds elapsed in evaluation: 121.60, eval metric: 0.00%\n",
      "INFO:tensorflow:Processed 178 documents\n",
      "INFO:tensorflow:Processed 1022 documents\n",
      "INFO:tensorflow:Total processed documents: 1202\n",
      "INFO:tensorflow:num correct tokens: 0\n",
      "INFO:tensorflow:total tokens: 122137\n",
      "INFO:tensorflow:Seconds elapsed in evaluation: 315.91, eval metric: 0.00%\n",
      "INFO:tensorflow:Processed 177 documents\n",
      "INFO:tensorflow:Total processed documents: 1199\n",
      "INFO:tensorflow:num correct tokens: 1199\n",
      "INFO:tensorflow:total tokens: 120495\n",
      "INFO:tensorflow:Seconds elapsed in evaluation: 353.28, eval metric: 1.00%\n"
     ]
    }
   ],
   "source": [
    "! cat ./news_texts.txt | docker run --rm -i inemo/syntaxnet_rus > news_texts.conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk import DependencyGraph\n",
    "\n",
    "def read_connlu(file_name):\n",
    "    \"\"\"\n",
    "    Читает и парсит файл ф вормате CoNLL-U\n",
    "    с данными результатов синтаксического анализа SyntaxNet\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    sentence = []    \n",
    "    with open(file_name, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            line = line.strip()\n",
    "\n",
    "            # Пустая строка — конец предложения\n",
    "            if not line:\n",
    "                sentences.append(sentence)\n",
    "                sentence = []                \n",
    "                    \n",
    "            # Не комментарий                    \n",
    "            elif not line.startswith('#'):  \n",
    "                items = line.split(\"\\t\")\n",
    "                assert len(items) == 10\n",
    "                sentence.append(\"\\t\".join(items) + '\\n')  \n",
    "                \n",
    "    return sentences\n",
    "\n",
    "\n",
    "def iter_depency_graphs(sentences):\n",
    "    for n, sentence in enumerate(sentences):\n",
    "        yield n, DependencyGraph(tree_str=sentence, cell_separator='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Компания рассчитывает на решение по газовому спору с Украиной до конца декабря «Газпром» ожидает решения Стокгольмского арбитража по транзитному спору с украинским «Нафтогазом» не позднее февраля 2018 года, хотя раньше компания прогнозировала, что это произойдет до 30 ноября. Решение по взаимным искам об условиях поставок газа на Украину, по мнению монополии, суд может принять на месяц раньше — до декабря 2017 года. \n",
      "\n",
      "0\n",
      "=============\n",
      "(('рассчитывает', 'VERB'), 'nsubj', ('Компания', 'NOUN'))\n",
      "(('рассчитывает', 'VERB'), 'dobj', ('решение', 'NOUN'))\n",
      "(('решение', 'NOUN'), 'case', ('на', 'ADP'))\n",
      "(('решение', 'NOUN'), 'dobj', ('спору', 'NOUN'))\n",
      "(('спору', 'NOUN'), 'case', ('по', 'ADP'))\n",
      "(('спору', 'NOUN'), 'amod', ('газовому', 'ADJ'))\n",
      "(('спору', 'NOUN'), 'dobj', ('Украиной', 'NOUN'))\n",
      "(('Украиной', 'NOUN'), 'case', ('с', 'ADP'))\n",
      "(('рассчитывает', 'VERB'), 'parataxis', ('ожидает', 'VERB'))\n",
      "(('ожидает', 'VERB'), 'nmod', ('конца', 'NOUN'))\n",
      "(('конца', 'NOUN'), 'case', ('до', 'ADP'))\n",
      "(('конца', 'NOUN'), 'nmod', ('декабря', 'NOUN'))\n",
      "(('ожидает', 'VERB'), 'nsubj', ('«Газпром»', 'NOUN'))\n",
      "(('ожидает', 'VERB'), 'dobj', ('решения', 'NOUN'))\n",
      "(('решения', 'NOUN'), 'nmod', ('арбитража', 'NOUN'))\n",
      "(('арбитража', 'NOUN'), 'amod', ('Стокгольмского', 'ADJ'))\n",
      "(('арбитража', 'NOUN'), 'nmod', ('спору', 'NOUN'))\n",
      "(('спору', 'NOUN'), 'case', ('по', 'ADP'))\n",
      "(('спору', 'NOUN'), 'amod', ('транзитному', 'ADJ'))\n",
      "(('ожидает', 'VERB'), 'iobj', ('«Нафтогазом»', 'NOUN'))\n",
      "(('«Нафтогазом»', 'NOUN'), 'case', ('с', 'ADP'))\n",
      "(('«Нафтогазом»', 'NOUN'), 'amod', ('украинским', 'ADJ'))\n",
      "(('ожидает', 'VERB'), 'conj', ('может', 'VERB'))\n",
      "(('может', 'VERB'), 'advmod', ('позднее', 'ADV'))\n",
      "(('позднее', 'ADV'), 'neg', ('не', 'PART'))\n",
      "(('позднее', 'ADV'), 'nmod', ('февраля', 'NOUN'))\n",
      "(('может', 'VERB'), 'nsubj', ('2018', 'NUM'))\n",
      "(('может', 'VERB'), 'advmod', ('года,', 'ADV'))\n",
      "(('может', 'VERB'), 'advcl', ('прогнозировала,', 'VERB'))\n",
      "(('прогнозировала,', 'VERB'), 'mark', ('хотя', 'SCONJ'))\n",
      "(('прогнозировала,', 'VERB'), 'advmod', ('раньше', 'ADV'))\n",
      "(('прогнозировала,', 'VERB'), 'nsubj', ('компания', 'NOUN'))\n",
      "(('прогнозировала,', 'VERB'), 'cc', ('что', 'SCONJ'))\n",
      "(('прогнозировала,', 'VERB'), 'advcl', ('произойдет', 'VERB'))\n",
      "(('произойдет', 'VERB'), 'nsubj', ('это', 'NOUN'))\n",
      "(('произойдет', 'VERB'), 'nmod', ('ноября.', 'NOUN'))\n",
      "(('ноября.', 'NOUN'), 'case', ('до', 'ADP'))\n",
      "(('ноября.', 'NOUN'), 'nummod', ('30', 'NUM'))\n",
      "(('может', 'VERB'), 'nsubj', ('Решение', 'NOUN'))\n",
      "(('Решение', 'NOUN'), 'nmod', ('искам', 'NOUN'))\n",
      "(('искам', 'NOUN'), 'case', ('по', 'ADP'))\n",
      "(('искам', 'NOUN'), 'amod', ('взаимным', 'ADJ'))\n",
      "(('может', 'VERB'), 'nmod', ('условиях', 'NOUN'))\n",
      "(('условиях', 'NOUN'), 'case', ('об', 'ADP'))\n",
      "(('условиях', 'NOUN'), 'dobj', ('поставок', 'NOUN'))\n",
      "(('поставок', 'NOUN'), 'dobj', ('газа', 'NOUN'))\n",
      "(('поставок', 'NOUN'), 'dobj', ('Украину,', 'NOUN'))\n",
      "(('Украину,', 'NOUN'), 'case', ('на', 'ADP'))\n",
      "(('может', 'VERB'), 'parataxis', ('мнению', 'NOUN'))\n",
      "(('мнению', 'NOUN'), 'case', ('по', 'ADP'))\n",
      "(('мнению', 'NOUN'), 'nmod', ('монополии,', 'NOUN'))\n",
      "(('может', 'VERB'), 'nsubj', ('суд', 'NOUN'))\n",
      "(('может', 'VERB'), 'xcomp', ('принять', 'VERB'))\n",
      "(('принять', 'VERB'), 'nmod', ('месяц', 'NOUN'))\n",
      "(('месяц', 'NOUN'), 'case', ('на', 'ADP'))\n",
      "(('принять', 'VERB'), 'advmod', ('раньше', 'ADV'))\n",
      "(('принять', 'VERB'), 'nmod', ('—', 'PUNCT'))\n",
      "(('—', 'PUNCT'), 'nmod', ('декабря', 'NOUN'))\n",
      "(('декабря', 'NOUN'), 'case', ('до', 'ADP'))\n",
      "(('декабря', 'NOUN'), 'nmod', ('года.', 'NOUN'))\n",
      "(('года.', 'NOUN'), 'nummod', ('2017', 'NUM'))\n"
     ]
    }
   ],
   "source": [
    "sentences = read_connlu('news_texts.conll')\n",
    "\n",
    "print(news.text[0], '\\n')\n",
    "\n",
    "for n, graph in iter_depency_graphs(sentences[:1]):\n",
    "    print(\"{}\\n=============\".format(n))\n",
    "    for triple in graph.triples():\n",
    "        if triple:\n",
    "            print(triple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Выделяем данные по санкциям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "санкц... amod ADJ (какие санкциии)\n",
      "===============\n",
      "западные санкции (7), штрафные санкции (3), секторальные санкции (2), международные санкции (2), американские санкции, очередные санкции, экономические санкции, антироссийские санкции, персональные санкции, визовые санкции\n",
      "\n",
      "санкц... nmod|dobj NOUN (санкции of ...)\n",
      "===============\n",
      "санкции США (5), санкциями на территории (2), санкций против РФ (2), санкций против банка, санкций Перевод, санкциями против России, санкции за публикации, санкции о заказчике, санкциями стран, санкций для Газпрома, санкций ExxonMobil, санкций Газпром, санкции против Газпрома, санкции к компаниям, санкции за неготовность, санкций для монополии\n",
      "\n",
      "NOUN nmod|dobj санкц... (... of санкций)\n",
      "===============\n",
      "введения санкций (4), влияния санкций (4), в условиях санкций (3), редакция санкций, с ННК из-за санкций, правомерность санкций, на фоне санкций, риску санкций, последствия санкций, от проектов санкциями, волну санкций, развития санкции, эффект из-за санкций, вступления санкций, под виток санкций, ситуации под санкции\n",
      "\n",
      "VERB санкц...\n",
      "===============\n",
      "грозят санкциями (2), терять из-за санкций (2), сообщил из-за санкций (2), подпадающей под санкции (2), обнаружила из-за санкций, идут из-за санкций, почувствовали санкций, попавшего под санкции, осложняют санкции, находящегося под санкциями, заморожено из-за санкций, попал санкции, хочет из-за санкций, находящийся под санкциями, подпавшие под санкции, несмотря на санкции, окажут санкции, включать санкции, мотивировано санкциями, объявили санкций, закрыты из-за санкций, влияют санкции, пересматривает из-за санкций, начнет из-за санкций, использовать санкции, подпавший под санкции, оказались из-за санкций, попавшие под санкции, могут санкции, попал под санкции\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "s_amod = Counter()\n",
    "s_nmod_dobj = Counter()\n",
    "o_nmod_dobj = Counter()\n",
    "o_verb = Counter()\n",
    "\n",
    "def normalize_phrase(*words):\n",
    "    phrase = ' '.join(filter(None, words))\n",
    "    return {\n",
    "        'западных санкций': 'западные санкции',\n",
    "        'секторальных санкций': 'секторальные санкции',\n",
    "        'штрафных санкций': 'штрафные санкции',\n",
    "        'американских санкций': 'американские санкции',\n",
    "        'антироссийскими санкциями': 'антироссийские санкции',\n",
    "        'экономическими санкциями': 'экономические санкции',\n",
    "        'очередных санкций': 'очередные санкции',\n",
    "        'санкций США': 'санкции США'\n",
    "    }.get(phrase, phrase)\n",
    "\n",
    "\n",
    "def iter_graph_tripples(graph):\n",
    "    for (obj_word, obj_tag), pred, (subj_word, subj_tag) in graph.triples():\n",
    "        obj_word = obj_word.strip(':,.\"«»-–—')\n",
    "        subj_word = subj_word.strip(':,.\"«»-–—')\n",
    "        yield (obj_word, obj_tag), pred, (subj_word, subj_tag)\n",
    "\n",
    "        \n",
    "def find_by_pred(\n",
    "    graph, preds=None,\n",
    "    obj_word=None, obj_tag=None, \n",
    "    subj_word=None, subj_tag=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Ищет в синтаксическом графе тройку (target, target_pred, X)\n",
    "    И возвращает слово X\n",
    "    \"\"\"\n",
    "    find_subj = subj_word is None\n",
    "    if preds is not None and not isinstance(preds, set):\n",
    "        preds = {preds}\n",
    "        \n",
    "    for obj, pred, subj in iter_graph_tripples(graph):\n",
    "        _obj_word, _obj_tag = obj\n",
    "        _subj_word, _subj_tag = subj\n",
    "        \n",
    "        if (\n",
    "            (preds is None or pred in preds) and\n",
    "            (obj_word is None or obj_word == _obj_word) and\n",
    "            (obj_tag is None or obj_tag == _obj_tag) and\n",
    "            (subj_word is None or subj_word == _subj_word) and                \n",
    "            (subj_tag is None or subj_tag == _subj_tag)               \n",
    "        ):\n",
    "            return _subj_word if find_subj else _obj_word\n",
    "\n",
    "for n, graph in iter_depency_graphs(sentences):\n",
    "    for (obj_word, obj_tag), pred, (subj_word, subj_tag) in iter_graph_tripples(graph):\n",
    "        if obj_word.startswith('санкц'):\n",
    "            if pred == 'amod' and subj_tag == 'ADJ':\n",
    "                s_amod.update([normalize_phrase(subj_word, obj_word)])             \n",
    "            elif pred in ('nmod', 'dobj') and subj_tag == 'NOUN':\n",
    "                case = find_by_pred(graph, 'case', obj_word=subj_word)\n",
    "                s_nmod_dobj.update([normalize_phrase(obj_word, case, subj_word)])   \n",
    "                \n",
    "        elif subj_word.startswith('санкц'): \n",
    "            if pred in ('nmod', 'dobj')  and obj_tag == 'NOUN':\n",
    "                case_obj = find_by_pred(graph, 'case', obj_word=obj_word)\n",
    "                case_subj = find_by_pred(graph, 'case', obj_word=subj_word)\n",
    "                o_nmod_dobj.update([\n",
    "                    normalize_phrase(case_obj, obj_word, case_subj, subj_word)\n",
    "                ])   \n",
    "            elif obj_tag == 'VERB':\n",
    "                case = find_by_pred(graph, 'case', obj_word=subj_word)\n",
    "                o_verb.update([normalize_phrase(obj_word, case, subj_word)])              \n",
    "        \n",
    "                \n",
    "def print_counted_list(counter, title, count=30):\n",
    "    print('\\n' + title)\n",
    "    print('===============')\n",
    "    print(', '.join(\n",
    "        '{} ({})'.format(item, count) if count > 1 else item\n",
    "        for item, count in counter.most_common(count)\n",
    "    ))\n",
    "    \n",
    "print_counted_list(s_amod, 'санкц... amod ADJ (какие санкциии)')\n",
    "print_counted_list(s_nmod_dobj, 'санкц... nmod|dobj NOUN (санкции of ...)')   \n",
    "print_counted_list(o_nmod_dobj, 'NOUN nmod|dobj санкц... (... of санкций)')\n",
    "print_counted_list(o_verb, 'VERB санкц...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** международные-американские-западные экономические санкции в отношении РФ и визовые санкции для отдельных персоналий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. Классификационная\n",
    "Вам предстоит решить следующую задачу: по текстам новостей за день определить, вырастет или понизится цена закрытия.\n",
    "Для этого:\n",
    "* бинаризуйте признак \"цена закрытия\":  новый признак ClosingPrice_bin равен 1, если по сравнению со вчера цена не упала, и 0 – в обратном случаея;\n",
    "* составьте бучающее и тестовое множество: данные до начала 2016 года используются для обучения, данные с 2016 года и позже – для тестирования.\n",
    "\n",
    "Таким образом, в каждлый момент времени мы знаем: \n",
    "* ClosingPrice_bin – бинарый целевой признак\n",
    "* слова из статей, опубликованных в этот день – объясняющие признаки\n",
    "\n",
    "В этой части задания вам нужно сделать baseline алгоритм и попытаться его улучшить в следующей части. \n",
    "\n",
    "Используйте любой известный вам алгоритм классификации текстов для того, Используйте $tf-idf$ преобразование, сингулярное разложение, нормировку признакого пространства и любые другие техники обработки данных, которые вы считаете нужным. Используйте accuracy и F-measure для оценки качества классификации. Покажите, как  $tf-idf$ преобразование или сингулярное разложение или любая другая использованная вами техника влияет на качество классификации.\n",
    "Если у выбранного вами алгоритма есть гиперпараметры (например, $\\alpha$ в преобразовании Лапласа для метода наивного Байеса), покажите, как изменение гиперпараметра влияет на качество классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>ClosingPrice</th>\n",
       "      <th>DailyHigh</th>\n",
       "      <th>DailyLow</th>\n",
       "      <th>VolumePcs</th>\n",
       "      <th>ClosingPrice_bin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-12-08</th>\n",
       "      <td>133.429993</td>\n",
       "      <td>132.600006</td>\n",
       "      <td>133.899994</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>16037970</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-07</th>\n",
       "      <td>133.699997</td>\n",
       "      <td>133.020004</td>\n",
       "      <td>133.869995</td>\n",
       "      <td>132.809998</td>\n",
       "      <td>18198430</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-06</th>\n",
       "      <td>133.330002</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>134.289993</td>\n",
       "      <td>132.910004</td>\n",
       "      <td>14641730</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-05</th>\n",
       "      <td>133.479996</td>\n",
       "      <td>133.649994</td>\n",
       "      <td>133.990005</td>\n",
       "      <td>132.779999</td>\n",
       "      <td>12684800</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-04</th>\n",
       "      <td>133.009995</td>\n",
       "      <td>133.770004</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>131.929993</td>\n",
       "      <td>17818980</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open  ClosingPrice   DailyHigh    DailyLow  VolumePcs  \\\n",
       "date                                                                      \n",
       "2017-12-08  133.429993    132.600006  133.899994  132.000000   16037970   \n",
       "2017-12-07  133.699997    133.020004  133.869995  132.809998   18198430   \n",
       "2017-12-06  133.330002    134.000000  134.289993  132.910004   14641730   \n",
       "2017-12-05  133.479996    133.649994  133.990005  132.779999   12684800   \n",
       "2017-12-04  133.009995    133.770004  134.000000  131.929993   17818980   \n",
       "\n",
       "            ClosingPrice_bin  \n",
       "date                          \n",
       "2017-12-08                 0  \n",
       "2017-12-07                 0  \n",
       "2017-12-06                 1  \n",
       "2017-12-05                 1  \n",
       "2017-12-04                 1  "
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Добавляем бинарный признак уменьшения стоимости акции за день\n",
    "prices['ClosingPrice_bin'] = prices.apply(lambda row: 1 if row.ClosingPrice >= row.Open else 0, axis=1)\n",
    "prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Новостей в общем диапозоне дат 1202\n",
      "Данных по ценам в общем диапозоне дат 1967\n",
      "Данных с ценами и новостями 1159\n"
     ]
    }
   ],
   "source": [
    "# Оставляем даты на которые известны и цены и новости\n",
    "common_prices, common_news = filter_common_dates(prices, news, date_from, date_to)\n",
    "print(\"Новостей в общем диапозоне дат\", common_news.shape[0])\n",
    "print(\"Данных по ценам в общем диапозоне дат\", common_prices.shape[0])\n",
    "\n",
    "common_news = common_news.loc[common_prices.index].dropna()\n",
    "closing_prices_bin = common_prices.loc[common_news.index].ClosingPrice_bin\n",
    "print(\"Данных с ценами и новостями\", closing_prices_bin.shape[0])\n",
    "\n",
    "X = common_news.text_norm\n",
    "y = closing_prices_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(901, 901, 258, 258)"
      ]
     },
     "execution_count": 826,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Делим на обучающую и тестовую выборки\n",
    "def split_data_by_year(X, y, year=2016):\n",
    "    train_idx = X.index.year < year\n",
    "    test_idx = X.index.year >= year\n",
    "    X_train = X.loc[train_idx]\n",
    "    X_test = X.loc[test_idx]\n",
    "    y_train = y.loc[train_idx]    \n",
    "    y_test = y.loc[test_idx]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data_by_year(X, y)\n",
    "X_train.shape[0], y_train.shape[0], X_test.shape[0], y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_regex = re.compile(r\"[а-яё][а-яё\\-–]+\", re.UNICODE | re.MULTILINE | re.IGNORECASE)\n",
    "stop_words = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 908,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "def predict_and_score(clf, X_train, X_test, y_train, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_predict = clf.predict(X_test)\n",
    "    print(\"Accuracy:    {0:.2f}\".format(accuracy_score(y_test, y_predict)))  \n",
    "    print(\"F1-measure:  {0:.2f}\".format(f1_score(y_test, y_predict)))  #, average='macro'\n",
    "    print(\"Precision:   {0:.2f}\".format(precision_score(y_test, y_predict)))\n",
    "    print(\"Recall:      {0:.2f}\".format(recall_score(y_test, y_predict)))    \n",
    "    return y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 915,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tune_model(pipeline, parameters, X, y, n_iter=50, cv=2, scoring='accuracy'):\n",
    "    grid_search = RandomizedSearchCV(\n",
    "        pipeline, parameters, \n",
    "        cv=cv, scoring=scoring, n_iter=n_iter, \n",
    "        random_state=random_state, n_jobs=1\n",
    "    )\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    print(\"Лучший результат: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Лучшие параметры:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        \n",
    "    print(\"\\nНа тестовых данных:\" % grid_search.best_score_)\n",
    "    predict_and_score(grid_search.best_estimator_, *split_data_by_year(X, y));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучший результат: 0.514\n",
      "Лучшие параметры:\n",
      "\tclf__alpha: 1e-06\n",
      "\tclf__max_iter: 20\n",
      "\tclf__penalty: 'l2'\n",
      "\treducer__n_components: 20\n",
      "\treducer__n_iter: 10\n",
      "\ttfidf__norm: 'l2'\n",
      "\ttfidf__smooth_idf: True\n",
      "\ttfidf__sublinear_tf: True\n",
      "\ttfidf__use_idf: False\n",
      "\tvect__max_df: 0.6\n",
      "\tvect__min_df: 1\n",
      "\tvect__ngram_range: (1, 1)\n",
      "\tvect__stop_words: None\n",
      "\tvect__token_pattern: re.compile('[а-яё][а-яё\\\\-–]+', re.IGNORECASE|re.MULTILINE)\n",
      "\n",
      "На тестовых данных:\n",
      "Accuracy:    0.52\n",
      "F1-measure:  0.31\n",
      "Precision:   0.46\n",
      "Recall:      0.23\n"
     ]
    }
   ],
   "source": [
    "tune_model(\n",
    "    Pipeline([\n",
    "        ('vect', CountVectorizer(stop_words=stop_words)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('reducer', TruncatedSVD(random_state=random_state)),\n",
    "        ('normalizer', StandardScaler()),\n",
    "        ('clf', SGDClassifier(random_state=random_state))\n",
    "    ]),\n",
    "    {\n",
    "        'vect__token_pattern': [rus_regex],\n",
    "        'vect__stop_words': [stop_words, None], \n",
    "        'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "        'vect__max_df': [0.6, 0.7, 0.75, 0.8],\n",
    "        'vect__min_df': [1, 2, 3, 4],\n",
    "        'tfidf__norm': ['l1', 'l2'],\n",
    "        'tfidf__use_idf': [True, False], \n",
    "        'tfidf__smooth_idf': [True, False],     \n",
    "        'tfidf__sublinear_tf': [True, False],       \n",
    "        'reducer__n_components': [10, 20, 30, 40, 50],\n",
    "        'reducer__n_iter': [10, 20, 30, 40, 50],  \n",
    "        'clf__penalty': ['elasticnet', 'l2'],\n",
    "        'clf__max_iter': [5, 10, 20, 40],    \n",
    "        'clf__alpha': [0.00001, 0.000001, 0.0000001],  \n",
    "    },\n",
    "    X, y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 987,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:    0.57\n",
      "F1-measure:  0.54\n",
      "Precision:   0.53\n",
      "Recall:      0.55\n"
     ]
    }
   ],
   "source": [
    "# Пытаемся ещё улучшть врчную\n",
    "predict_and_score(\n",
    "    Pipeline([\n",
    "        ('vect', CountVectorizer(\n",
    "            ngram_range=(1, 2), \n",
    "            max_df=0.75, \n",
    "            min_df=2, \n",
    "            token_pattern=rus_regex,\n",
    "        )),\n",
    "        ('tfidf', TfidfTransformer(smooth_idf=True, sublinear_tf=False, use_idf=False)),\n",
    "        ('reducer', TruncatedSVD(n_components=40, n_iter=50, random_state=random_state)),\n",
    "        ('normalizer', StandardScaler()),\n",
    "        ('clf', SGDClassifier(random_state=random_state, penalty='elasticnet', max_iter=20, alpha=1e-07))\n",
    "    ]),\n",
    "    X_train, X_test, y_train, y_test\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 989,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Без TF-IDF\n",
      "Accuracy:    0.51\n",
      "F1-measure:  0.43\n",
      "Precision:   0.46\n",
      "Recall:      0.40\n",
      "\n",
      "Без TF-IDF и SVD\n",
      "Accuracy:    0.53\n",
      "F1-measure:  0.50\n",
      "Precision:   0.48\n",
      "Recall:      0.52\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nБез TF-IDF\")\n",
    "predict_and_score(\n",
    "    Pipeline([\n",
    "        ('vect', CountVectorizer(\n",
    "            ngram_range=(1, 2), \n",
    "            max_df=0.75, \n",
    "            min_df=2, \n",
    "            token_pattern=rus_regex,\n",
    "        )),\n",
    "        ('reducer', TruncatedSVD(n_components=40, n_iter=50, random_state=random_state)),\n",
    "        ('normalizer', StandardScaler()),\n",
    "        ('clf', SGDClassifier(random_state=random_state, penalty='elasticnet', max_iter=20, alpha=1e-07))\n",
    "    ]),\n",
    "    X_train, X_test, y_train, y_test\n",
    ");\n",
    "\n",
    "print(\"\\nБез TF-IDF и SVD\")\n",
    "predict_and_score(\n",
    "    Pipeline([\n",
    "        ('vect', CountVectorizer(\n",
    "            ngram_range=(1, 2), \n",
    "            max_df=0.75, \n",
    "            min_df=2, \n",
    "            token_pattern=rus_regex,\n",
    "        )),\n",
    "        ('clf', SGDClassifier(random_state=random_state, penalty='elasticnet', max_iter=20, alpha=1e-07))\n",
    "    ]),\n",
    "    X_train, X_test, y_train, y_test\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 967,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:    0.56\n",
      "F1-measure:  0.40\n",
      "Precision:   0.54\n",
      "Recall:      0.32\n"
     ]
    }
   ],
   "source": [
    "predict_and_score(\n",
    "    Pipeline([\n",
    "        ('vect', CountVectorizer(\n",
    "            ngram_range=(1, 2), \n",
    "            max_df=0.75, \n",
    "            min_df=3, \n",
    "            stop_words=stop_words, \n",
    "            token_pattern=rus_regex,\n",
    "        )),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('reducer', TruncatedSVD(n_components=40, n_iter=50, random_state=random_state)),\n",
    "        ('clf', RandomForestClassifier(\n",
    "            n_estimators=130, \n",
    "            min_samples_leaf=2,\n",
    "            min_samples_split=2,\n",
    "            max_depth=5,\n",
    "            random_state=random_state\n",
    "        )),\n",
    "    ]),\n",
    "    X_train, X_test, y_train, y_test\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 970,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Без TF-IDF\n",
      "Accuracy:    0.50\n",
      "F1-measure:  0.32\n",
      "Precision:   0.42\n",
      "Recall:      0.25\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nБез TF-IDF\")\n",
    "predict_and_score(\n",
    "    Pipeline([\n",
    "        ('vect', CountVectorizer(\n",
    "            ngram_range=(1, 2), \n",
    "            max_df=0.75, \n",
    "            min_df=3, \n",
    "            stop_words=stop_words, \n",
    "            token_pattern=rus_regex,\n",
    "        )),\n",
    "        ('reducer', TruncatedSVD(n_components=40, n_iter=50, random_state=random_state)),\n",
    "        ('clf', RandomForestClassifier(\n",
    "            n_estimators=130, \n",
    "            min_samples_leaf=2,\n",
    "            min_samples_split=2,\n",
    "            max_depth=5,\n",
    "            random_state=random_state\n",
    "        )),\n",
    "    ]),\n",
    "    X_train, X_test, y_train, y_test\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы\n",
    "- Лучше всего сработал **`SGDClassifier`**. Так же тестировались **`MultinomialNB`** и **`LogisticRegression`**, результаты были ещё хуже **`RandomForestClassifier`**\n",
    "- В общем-то эффективность получившегося предсказателя близка к рандому :(\n",
    "- **`Tf-IDf`** + **`SVD`** немного, но улучшает качество"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Часть 3. Творческая\n",
    "Придумайте и попытайтесь сделать еще что-нибудь, чтобы улучшить качество классификации. \n",
    "Направления развития:\n",
    "* Морфологический признаки: \n",
    "    * использовать в качестве признаков только существительные или только именованные сущности;\n",
    "* Модели скрытых тем:\n",
    "    * использовать в качестве признаков скрытые темы;\n",
    "    * использовать в качестве признаков динамические скрытые темы \n",
    "    пример тут: (https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/dtm_example.ipynb)\n",
    "* Синтаксические признаки:\n",
    "    * использовать SOV-тройки в качестве признаков\n",
    "    * кластеризовать SOV-тройки по эмбеддингам глаголов (обученные word2vec модели можно скачать отсюда: (http://rusvectores.org/ru/models/) и использовать только центроиды кластеров в качестве признаков\n",
    "* что-нибудь еще     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
